{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['clip.code-workspace', 'Untitled-1.ipynb', 'script_identifier.pth']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.ImageFolder(root=\"/home/pup/Desktop/Script Recognition Data/dataset/test\", transform=transform)\n",
    "val_dataset = datasets.ImageFolder(root=\"/home/pup/Desktop/Script Recognition Data/dataset/test\", transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pup/anaconda3/envs/CLIP/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/pup/anaconda3/envs/CLIP/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = models.resnet18(pretrained=True)\n",
    "model.fc = nn.Linear(512, len(train_dataset.classes))  # Modify for number of classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Move to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/100], Loss: 0.0002\n"
     ]
    }
   ],
   "source": [
    "print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Complete!\n"
     ]
    }
   ],
   "source": [
    "# Save trained model\n",
    "torch.save(model.state_dict(), \"script_identifier.pth\")\n",
    "\n",
    "print(\"Training Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model ready! Number of classes: 10\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained ResNet-18 model\n",
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "# Modify the final classification layer to match your number of script classes\n",
    "num_classes = len(train_dataset.classes)  # Automatically detects number of classes\n",
    "model.fc = nn.Linear(512, num_classes)  \n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "print(f\"Model ready! Number of classes: {num_classes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 0.6353, Accuracy: 79.77%\n",
      "Epoch [2/100], Loss: 0.2732, Accuracy: 91.93%\n",
      "Epoch [3/100], Loss: 0.1168, Accuracy: 96.16%\n",
      "Epoch [4/100], Loss: 0.2119, Accuracy: 93.34%\n",
      "Epoch [5/100], Loss: 0.1507, Accuracy: 95.90%\n",
      "Epoch [6/100], Loss: 0.1080, Accuracy: 96.80%\n",
      "Epoch [7/100], Loss: 0.1594, Accuracy: 94.88%\n",
      "Epoch [8/100], Loss: 0.2024, Accuracy: 95.26%\n",
      "Epoch [9/100], Loss: 0.1060, Accuracy: 96.67%\n",
      "Epoch [10/100], Loss: 0.0830, Accuracy: 97.44%\n",
      "Epoch [11/100], Loss: 0.0998, Accuracy: 97.31%\n",
      "Epoch [12/100], Loss: 0.0671, Accuracy: 97.44%\n",
      "Epoch [13/100], Loss: 0.0830, Accuracy: 97.70%\n",
      "Epoch [14/100], Loss: 0.0908, Accuracy: 97.06%\n",
      "Epoch [15/100], Loss: 0.0612, Accuracy: 98.34%\n",
      "Epoch [16/100], Loss: 0.0484, Accuracy: 98.59%\n",
      "Epoch [17/100], Loss: 0.0356, Accuracy: 99.10%\n",
      "Epoch [18/100], Loss: 0.0433, Accuracy: 98.85%\n",
      "Epoch [19/100], Loss: 0.0354, Accuracy: 98.85%\n",
      "Epoch [20/100], Loss: 0.0245, Accuracy: 99.10%\n",
      "Epoch [21/100], Loss: 0.0302, Accuracy: 98.98%\n",
      "Epoch [22/100], Loss: 0.0174, Accuracy: 99.10%\n",
      "Epoch [23/100], Loss: 0.0123, Accuracy: 99.49%\n",
      "Epoch [24/100], Loss: 0.0045, Accuracy: 100.00%\n",
      "Epoch [25/100], Loss: 0.0119, Accuracy: 99.62%\n",
      "Epoch [26/100], Loss: 0.0047, Accuracy: 100.00%\n",
      "Epoch [27/100], Loss: 0.0043, Accuracy: 99.87%\n",
      "Epoch [28/100], Loss: 0.0061, Accuracy: 99.87%\n",
      "Epoch [29/100], Loss: 0.0014, Accuracy: 100.00%\n",
      "Epoch [30/100], Loss: 0.0060, Accuracy: 99.74%\n",
      "Epoch [31/100], Loss: 0.0153, Accuracy: 99.49%\n",
      "Epoch [32/100], Loss: 0.0556, Accuracy: 98.21%\n",
      "Epoch [33/100], Loss: 0.1286, Accuracy: 96.67%\n",
      "Epoch [34/100], Loss: 0.1125, Accuracy: 96.29%\n",
      "Epoch [35/100], Loss: 0.0445, Accuracy: 98.85%\n",
      "Epoch [36/100], Loss: 0.0643, Accuracy: 98.72%\n",
      "Epoch [37/100], Loss: 0.0632, Accuracy: 97.95%\n",
      "Epoch [38/100], Loss: 0.0341, Accuracy: 98.72%\n",
      "Epoch [39/100], Loss: 0.0461, Accuracy: 98.72%\n",
      "Epoch [40/100], Loss: 0.0292, Accuracy: 98.85%\n",
      "Epoch [41/100], Loss: 0.0359, Accuracy: 98.85%\n",
      "Epoch [42/100], Loss: 0.0373, Accuracy: 98.46%\n",
      "Epoch [43/100], Loss: 0.0369, Accuracy: 98.98%\n",
      "Epoch [44/100], Loss: 0.0637, Accuracy: 98.21%\n",
      "Epoch [45/100], Loss: 0.0569, Accuracy: 98.21%\n",
      "Epoch [46/100], Loss: 0.0620, Accuracy: 97.82%\n",
      "Epoch [47/100], Loss: 0.0709, Accuracy: 98.34%\n",
      "Epoch [48/100], Loss: 0.0493, Accuracy: 98.46%\n",
      "Epoch [49/100], Loss: 0.0118, Accuracy: 99.74%\n",
      "Epoch [50/100], Loss: 0.0217, Accuracy: 99.10%\n",
      "Epoch [51/100], Loss: 0.0724, Accuracy: 98.34%\n",
      "Epoch [52/100], Loss: 0.1604, Accuracy: 96.03%\n",
      "Epoch [53/100], Loss: 0.1861, Accuracy: 94.88%\n",
      "Epoch [54/100], Loss: 0.0540, Accuracy: 98.85%\n",
      "Epoch [55/100], Loss: 0.0389, Accuracy: 98.72%\n",
      "Epoch [56/100], Loss: 0.0337, Accuracy: 98.98%\n",
      "Epoch [57/100], Loss: 0.0604, Accuracy: 98.34%\n",
      "Epoch [58/100], Loss: 0.0266, Accuracy: 99.10%\n",
      "Epoch [59/100], Loss: 0.0793, Accuracy: 98.72%\n",
      "Epoch [60/100], Loss: 0.0347, Accuracy: 99.10%\n",
      "Epoch [61/100], Loss: 0.0285, Accuracy: 98.98%\n",
      "Epoch [62/100], Loss: 0.0248, Accuracy: 98.98%\n",
      "Epoch [63/100], Loss: 0.0099, Accuracy: 99.74%\n",
      "Epoch [64/100], Loss: 0.0416, Accuracy: 99.62%\n",
      "Epoch [65/100], Loss: 0.0369, Accuracy: 98.85%\n",
      "Epoch [66/100], Loss: 0.0121, Accuracy: 99.49%\n",
      "Epoch [67/100], Loss: 0.0174, Accuracy: 99.23%\n",
      "Epoch [68/100], Loss: 0.0492, Accuracy: 98.46%\n",
      "Epoch [69/100], Loss: 0.0046, Accuracy: 100.00%\n",
      "Epoch [70/100], Loss: 0.0145, Accuracy: 99.49%\n",
      "Epoch [71/100], Loss: 0.0324, Accuracy: 99.10%\n",
      "Epoch [72/100], Loss: 0.0301, Accuracy: 99.62%\n",
      "Epoch [73/100], Loss: 0.0074, Accuracy: 99.87%\n",
      "Epoch [74/100], Loss: 0.0054, Accuracy: 99.87%\n",
      "Epoch [75/100], Loss: 0.0044, Accuracy: 100.00%\n",
      "Epoch [76/100], Loss: 0.0047, Accuracy: 99.87%\n",
      "Epoch [77/100], Loss: 0.0035, Accuracy: 99.74%\n",
      "Epoch [78/100], Loss: 0.0023, Accuracy: 99.87%\n",
      "Epoch [79/100], Loss: 0.0006, Accuracy: 100.00%\n",
      "Epoch [80/100], Loss: 0.0005, Accuracy: 100.00%\n",
      "Epoch [81/100], Loss: 0.0006, Accuracy: 100.00%\n",
      "Epoch [82/100], Loss: 0.0003, Accuracy: 100.00%\n",
      "Epoch [83/100], Loss: 0.0017, Accuracy: 99.87%\n",
      "Epoch [84/100], Loss: 0.0003, Accuracy: 100.00%\n",
      "Epoch [85/100], Loss: 0.0015, Accuracy: 100.00%\n",
      "Epoch [86/100], Loss: 0.0018, Accuracy: 100.00%\n",
      "Epoch [87/100], Loss: 0.0002, Accuracy: 100.00%\n",
      "Epoch [88/100], Loss: 0.0006, Accuracy: 100.00%\n",
      "Epoch [89/100], Loss: 0.0012, Accuracy: 100.00%\n",
      "Epoch [90/100], Loss: 0.0013, Accuracy: 100.00%\n",
      "Epoch [91/100], Loss: 0.0003, Accuracy: 100.00%\n",
      "Epoch [92/100], Loss: 0.0006, Accuracy: 100.00%\n",
      "Epoch [93/100], Loss: 0.0007, Accuracy: 100.00%\n",
      "Epoch [94/100], Loss: 0.0236, Accuracy: 99.36%\n",
      "Epoch [95/100], Loss: 0.0118, Accuracy: 99.74%\n",
      "Epoch [96/100], Loss: 0.0073, Accuracy: 99.87%\n",
      "Epoch [97/100], Loss: 0.0052, Accuracy: 99.74%\n",
      "Epoch [98/100], Loss: 0.0186, Accuracy: 99.36%\n",
      "Epoch [99/100], Loss: 0.0074, Accuracy: 99.62%\n",
      "Epoch [100/100], Loss: 0.0083, Accuracy: 99.62%\n",
      "Training Complete!\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    train_acc = 100 * correct / total\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}, Accuracy: {train_acc:.2f}%\")\n",
    "\n",
    "print(\"Training Complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"script_identifier.pth\")\n",
    "print(\"Model saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 97.70%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "val_acc = 100 * correct / total\n",
    "print(f\"Validation Accuracy: {val_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Script: 4\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Load class names\n",
    "class_names = train_dataset.classes\n",
    "\n",
    "# Load an image for testing\n",
    "def predict_script(image_path):\n",
    "    image = Image.open(image_path)\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.Resize((128, 128)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "    \n",
    "    image = transform_test(image).unsqueeze(0).to(device)  # Add batch dimension\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "        _, predicted = output.max(1)\n",
    "    \n",
    "    print(f\"Predicted Script: {class_names[predicted.item()]}\")\n",
    "\n",
    "# Example usage\n",
    "predict_script(\"/home/pup/Downloads/Punjabi/HW_WORDS_Punjabi/test/images/1.jpg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9795\n",
      "Precision: 0.9820\n",
      "Recall: 0.9795\n",
      "F1 Score: 0.9796\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load class names\n",
    "class_names = train_dataset.classes\n",
    "\n",
    "def predict_script(image_path, model, device):\n",
    "    image = Image.open(image_path).convert('RGB')  # Ensure 3 channels\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.Resize((128, 128)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "    \n",
    "    image = transform_test(image).unsqueeze(0).to(device)  # Add batch dimension\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "        _, predicted = output.max(1)\n",
    "    \n",
    "    return predicted.item()\n",
    "\n",
    "def evaluate_model(test_folder, model, device):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    for label in os.listdir(test_folder):\n",
    "        label_path = os.path.join(test_folder, label)\n",
    "        if not os.path.isdir(label_path):\n",
    "            continue\n",
    "        \n",
    "        for img_name in os.listdir(label_path):\n",
    "            img_path = os.path.join(label_path, img_name)\n",
    "            if img_path.lower().endswith(('png', 'jpg', 'jpeg')):\n",
    "                predicted_label = predict_script(img_path, model, device)\n",
    "                y_pred.append(predicted_label)\n",
    "                y_true.append(class_names.index(label))\n",
    "    \n",
    "    # Compute metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='weighted')\n",
    "    recall = recall_score(y_true, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Example usage\n",
    "test_folder = \"/home/pup/Desktop/Script Recognition Data/dataset/test\"  # Folder structure: test_folder/class_name/image.jpg\n",
    "evaluate_model(test_folder, model, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CLIP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
